{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13577355,"sourceType":"datasetVersion","datasetId":8625521},{"sourceId":13578071,"sourceType":"datasetVersion","datasetId":8626040}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# --- Step 1.1: Setup and Ingestion (Safe Version) ---\n\n# 1. Install required libraries\nprint(\"Installing libraries... (This may take a minute)\")\n!pip install moviepy -q\n!pip install scenedetect -q\n!pip install opencv-python -q\nprint(\"‚úÖ Libraries installed successfully.\")\n\n# 2. Import all necessary libraries for the project\nimport os\nimport glob\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom moviepy.editor import VideoFileClip, concatenate_videoclips, AudioFileClip\n\n# Now this import will work!\nfrom scenedetect import VideoManager, SceneManager\nfrom scenedetect.detectors import ContentDetector\n# --- FIX ---\n# Removed all specific function imports from scene_manager\n# as they are not needed for Step 1.2\n# --- END FIX ---\nfrom tqdm.notebook import tqdm # For a nice progress bar\n\nprint(\"‚úÖ All libraries imported.\")\n\n# 3. Define file paths\nINPUT_DATASET_PATH = \"/kaggle/input/short-clips/dataset1/\"\nOUTPUT_DIR = \"/kaggle/working/\"\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# 4. Find all video clips to process\nvideo_files = glob.glob(os.path.join(INPUT_DATASET_PATH, \"*.mp4\"))\nvideo_files.sort() # Sort them to process in a consistent order\n\n# 5. Confirmation\nif video_files:\n    print(f\"‚úÖ Found {len(video_files)} video files to process:\")\n    for f in video_files:\n        print(f\"  - {os.path.basename(f)}\")\nelse:\n    print(f\"‚ö†Ô∏è Warning: No .mp4 files found in {INPUT_DATASET_PATH}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-01T13:24:38.588749Z","iopub.execute_input":"2025-11-01T13:24:38.589413Z","iopub.status.idle":"2025-11-01T13:24:49.365097Z","shell.execute_reply.started":"2025-11-01T13:24:38.589390Z","shell.execute_reply":"2025-11-01T13:24:49.364033Z"}},"outputs":[{"name":"stdout","text":"Installing libraries... (This may take a minute)\n‚úÖ Libraries installed successfully.\n","output_type":"stream"},{"name":"stderr","text":"\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.6 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\nTraceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n    ColabKernelApp.launch_instance()\n  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 211, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n    await self.process_one()\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n    await dispatch(*args)\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n    await result\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n    reply_content = await reply_content\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n    res = shell.run_cell(\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n    result = self._run_cell(\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n    return runner(coro)\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_120/4059193500.py\", line 16, in <cell line: 0>\n    from moviepy.editor import VideoFileClip, concatenate_videoclips, AudioFileClip\n  File \"/usr/local/lib/python3.11/dist-packages/moviepy/editor.py\", line 60, in <module>\n    from .video.io.sliders import sliders\n  File \"/usr/local/lib/python3.11/dist-packages/moviepy/video/io/sliders.py\", line 1, in <module>\n    import matplotlib.pyplot as plt\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/__init__.py\", line 129, in <module>\n    from . import _api, _version, cbook, _docstring, rcsetup\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/rcsetup.py\", line 27, in <module>\n    from matplotlib.colors import Colormap, is_color_like\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py\", line 56, in <module>\n    from matplotlib import _api, _cm, cbook, scale\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/scale.py\", line 22, in <module>\n    from matplotlib.ticker import (\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/ticker.py\", line 138, in <module>\n    from matplotlib import transforms as mtransforms\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/transforms.py\", line 49, in <module>\n    from matplotlib._path import (\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"],"ename":"AttributeError","evalue":"_ARRAY_API not found","output_type":"error"},{"name":"stderr","text":"error: XDG_RUNTIME_DIR not set in the environment.\nALSA lib confmisc.c:855:(parse_card) cannot find card '0'\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\nALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\nALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\nALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\nALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\nALSA lib confmisc.c:855:(parse_card) cannot find card '0'\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\nALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings","output_type":"stream"},{"name":"stdout","text":"‚úÖ All libraries imported.\n‚úÖ Found 13 video files to process:\n  - asconvpt.mp4\n  - blostjdq.mp4\n  - chqblpze.mp4\n  - frbnwkkq.mp4\n  - kjhjjxni.mp4\n  - myqiuzzc.mp4\n  - ncikuizq.mp4\n  - ohwchuju.mp4\n  - ukonzgxq.mp4\n  - unygqzdu.mp4\n  - wcsdambp.mp4\n  - wuqoarpl.mp4\n  - wylqhkno.mp4\n","output_type":"stream"},{"name":"stderr","text":"\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\nALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\nALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\nALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# --- Step 1.2: Shot Detection & Analysis Pipeline ---\n\n# This list will hold all the data about every shot\nall_shots_data = []\n\ndef find_shots(video_path):\n    \"\"\"\n    Uses PySceneDetect to find all the shots in a single video file.\n    Returns a list of (start_frame, end_frame) tuples.\n    \"\"\"\n    try:\n        video_manager = VideoManager([video_path])\n        scene_manager = SceneManager()\n        scene_manager.add_detector(ContentDetector(threshold=27.0))\n        \n        # This part does the work\n        video_manager.set_downscale_factor(1) # Process at full resolution\n        video_manager.start()\n        scene_manager.detect_scenes(frame_source=video_manager)\n        \n        # Get the list of scenes (shots)\n        scene_list = scene_manager.get_scene_list()\n        \n        # Handle case where no scenes are detected (the whole clip is one shot)\n        if not scene_list:\n            # Get total frames from the video_manager\n            total_frames = int(video_manager.get_duration().get_frames())\n            if total_frames > 0:\n                return [(video_manager.get_framerate().get_timecode(0), \n                         video_manager.get_duration())]\n            else:\n                return []\n                \n        video_manager.release()\n        return scene_list\n        \n    except Exception as e:\n        print(f\"Error processing {video_path}: {e}\")\n        return []\n\ndef get_blur_score(frame):\n    \"\"\"\n    Calculates the blurriness of a single frame using Laplacian variance.\n    A higher score is SHARPER. A lower score is BLURRIER.\n    \"\"\"\n    # Convert to grayscale\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n    # Calculate the variance of the Laplacian\n    variance = cv2.Laplacian(gray, cv2.CV_64F).var()\n    return variance\n\n# --- Main Processing Loop ---\n# This will go through every video file one by one.\nprint(f\"Starting analysis of {len(video_files)} files...\")\n\nfor video_path in tqdm(video_files, desc=\"Processing Videos\"):\n    video_filename = os.path.basename(video_path)\n    print(f\"\\nProcessing: {video_filename}\")\n    \n    shots = find_shots(video_path)\n    \n    if not shots:\n        print(f\"  -> No shots found, skipping.\")\n        continue\n        \n    print(f\"  -> Found {len(shots)} shots.\")\n    \n    # Now we analyze each shot in this video\n    cap = cv2.VideoCapture(video_path)\n    \n    for i, (start_time, end_time) in enumerate(tqdm(shots, desc=\"Analyzing Shots\")):\n        start_frame = start_time.get_frames()\n        end_frame = end_time.get_frames()\n        \n        # Get the middle frame of the shot for analysis\n        # This is more efficient than analyzing every single frame\n        middle_frame_num = int((start_frame + end_frame) / 2)\n        cap.set(cv2.CAP_PROP_POS_FRAMES, middle_frame_num)\n        \n        ret, frame = cap.read()\n        \n        if ret and frame is not None:\n            # --- Analyze the frame ---\n            blur_score = get_blur_score(frame)\n            \n            # --- Store the results ---\n            shot_data = {\n                'video_file': video_filename,\n                'video_path': video_path,\n                'shot_num': i,\n                'start_timecode': start_time.get_timecode(),\n                'end_timecode': end_time.get_timecode(),\n                'duration_sec': (end_frame - start_frame) / start_time.framerate,\n                'blur_score': blur_score\n                # We will add 'face_score', 'shake_score', etc. here\n            }\n            all_shots_data.append(shot_data)\n        \n    cap.release()\n\nprint(\"\\n--- Analysis Complete ---\")\n\n# --- Create the DataFrame ---\n# This is our key \"pipeline\" artifact\ndf_shots = pd.DataFrame(all_shots_data)\n\n# Let's look at the results\nprint(f\"‚úÖ Created DataFrame with {len(df_shots)} total shots.\")\nif not df_shots.empty:\n    print(df_shots.head())\n    \n    # Let's see the \"blurriest\" shots we found as a test\n    print(\"\\nBlurriest shots found:\")\n    print(df_shots.sort_values(by='blur_score').head())\n    \n    # Let's see the \"sharpest\" shots we found\n    print(\"\\nSharpest shots found:\")\n    print(df_shots.sort_values(by='blur_score', ascending=False).head())\nelse:\n    print(\"‚ö†Ô∏è Warning: The DataFrame is empty. No shots were analyzed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T13:24:49.366405Z","iopub.execute_input":"2025-11-01T13:24:49.366812Z","iopub.status.idle":"2025-11-01T13:26:03.103112Z","shell.execute_reply.started":"2025-11-01T13:24:49.366794Z","shell.execute_reply":"2025-11-01T13:26:03.102308Z"}},"outputs":[{"name":"stdout","text":"Starting analysis of 13 files...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing Videos:   0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61b7af5c35034b6e940abd79996c0a3c"}},"metadata":{}},{"name":"stdout","text":"\nProcessing: asconvpt.mp4\n  -> Found 4 shots.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Analyzing Shots:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa3f3b2657224e98ab5545c6b71afadb"}},"metadata":{}},{"name":"stdout","text":"\nProcessing: blostjdq.mp4\n  -> Found 6 shots.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Analyzing Shots:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb9243af0e2c4c279019f0bfc64a9716"}},"metadata":{}},{"name":"stdout","text":"\nProcessing: chqblpze.mp4\n  -> Found 13 shots.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Analyzing Shots:   0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afef746b2420448a94e9aa822f02de16"}},"metadata":{}},{"name":"stdout","text":"\nProcessing: frbnwkkq.mp4\nError processing /kaggle/input/short-clips/dataset1/frbnwkkq.mp4: 'tuple' object has no attribute 'get_frames'\n  -> No shots found, skipping.\n\nProcessing: kjhjjxni.mp4\n  -> Found 4 shots.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Analyzing Shots:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d10bace4bb7e490f94f8aa9f1ec0b115"}},"metadata":{}},{"name":"stdout","text":"\nProcessing: myqiuzzc.mp4\n  -> Found 3 shots.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Analyzing Shots:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a48e3ff648db4777b19f3e06dd9c537e"}},"metadata":{}},{"name":"stdout","text":"\nProcessing: ncikuizq.mp4\n  -> Found 4 shots.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Analyzing Shots:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acf1d40da17d4f5c91403ae91f229a10"}},"metadata":{}},{"name":"stdout","text":"\nProcessing: ohwchuju.mp4\n  -> Found 6 shots.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Analyzing Shots:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dead423efea5414db8a14d079993b847"}},"metadata":{}},{"name":"stdout","text":"\nProcessing: ukonzgxq.mp4\n  -> Found 2 shots.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Analyzing Shots:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2f898dc571f45eba700160ce68a1da4"}},"metadata":{}},{"name":"stdout","text":"\nProcessing: unygqzdu.mp4\n  -> Found 5 shots.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Analyzing Shots:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"267d50c7d9cb4dc183db09fdd5f0ca0a"}},"metadata":{}},{"name":"stdout","text":"\nProcessing: wcsdambp.mp4\n  -> Found 12 shots.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Analyzing Shots:   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85901410ae284e1795a75b87bdc262a0"}},"metadata":{}},{"name":"stdout","text":"\nProcessing: wuqoarpl.mp4\n  -> Found 16 shots.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Analyzing Shots:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"089ba38a2c3a4a2a8da2a43848c272a6"}},"metadata":{}},{"name":"stdout","text":"\nProcessing: wylqhkno.mp4\n  -> Found 4 shots.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Analyzing Shots:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb96d2329b744dd2b5ffe8654b92ef80"}},"metadata":{}},{"name":"stdout","text":"\n--- Analysis Complete ---\n‚úÖ Created DataFrame with 79 total shots.\n     video_file                                       video_path  shot_num  \\\n0  asconvpt.mp4  /kaggle/input/short-clips/dataset1/asconvpt.mp4         0   \n1  asconvpt.mp4  /kaggle/input/short-clips/dataset1/asconvpt.mp4         1   \n2  asconvpt.mp4  /kaggle/input/short-clips/dataset1/asconvpt.mp4         2   \n3  asconvpt.mp4  /kaggle/input/short-clips/dataset1/asconvpt.mp4         3   \n4  blostjdq.mp4  /kaggle/input/short-clips/dataset1/blostjdq.mp4         0   \n\n  start_timecode  end_timecode  duration_sec  blur_score  \n0   00:00:00.000  00:00:20.760         20.76   64.161774  \n1   00:00:20.760  00:00:59.300         38.54   89.137438  \n2   00:00:59.300  00:01:00.940          1.64   84.508437  \n3   00:01:00.940  00:01:01.880          0.94  202.184176  \n4   00:00:00.000  00:00:03.620          3.62  146.572058  \n\nBlurriest shots found:\n      video_file                                       video_path  shot_num  \\\n37  ohwchuju.mp4  /kaggle/input/short-clips/dataset1/ohwchuju.mp4         3   \n36  ohwchuju.mp4  /kaggle/input/short-clips/dataset1/ohwchuju.mp4         2   \n5   blostjdq.mp4  /kaggle/input/short-clips/dataset1/blostjdq.mp4         1   \n7   blostjdq.mp4  /kaggle/input/short-clips/dataset1/blostjdq.mp4         3   \n75  wylqhkno.mp4  /kaggle/input/short-clips/dataset1/wylqhkno.mp4         0   \n\n   start_timecode  end_timecode  duration_sec  blur_score  \n37   00:00:27.560  00:00:28.140          0.58    4.750798  \n36   00:00:27.080  00:00:27.560          0.48   11.674004  \n5    00:00:03.620  00:00:15.160         11.54   14.324140  \n7    00:00:21.080  00:00:29.920          8.84   27.893023  \n75   00:00:00.000  00:00:15.360         15.36   29.143153  \n\nSharpest shots found:\n      video_file                                       video_path  shot_num  \\\n24  kjhjjxni.mp4  /kaggle/input/short-clips/dataset1/kjhjjxni.mp4         1   \n27  myqiuzzc.mp4  /kaggle/input/short-clips/dataset1/myqiuzzc.mp4         0   \n28  myqiuzzc.mp4  /kaggle/input/short-clips/dataset1/myqiuzzc.mp4         1   \n47  wcsdambp.mp4  /kaggle/input/short-clips/dataset1/wcsdambp.mp4         0   \n48  wcsdambp.mp4  /kaggle/input/short-clips/dataset1/wcsdambp.mp4         1   \n\n   start_timecode  end_timecode  duration_sec  blur_score  \n24   00:00:03.040  00:00:05.480          2.44  957.518674  \n27   00:00:00.000  00:00:01.080          1.08  773.385042  \n28   00:00:01.080  00:00:03.980          2.90  757.628206  \n47   00:00:00.000  00:00:03.780          3.78  711.802823  \n48   00:00:03.780  00:00:07.380          3.60  663.589515  \n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# --- Step 1.3: Advanced Shot Analysis (Blur, Faces, Motion) ---\n# We already have the 'df_shots' DataFrame from the previous cell,\n# but we will rebuild it with all the new scores.\n\n# --- 1. Load Face Detection Model ---\n# This path points to the pre-installed file in the Kaggle environment\nface_cascade_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\nface_cascade = cv2.CascadeClassifier()\n\nif not face_cascade.load(face_cascade_path):\n    print(\"‚ö†Ô∏è Warning: Could not load face cascade classifier.\")\nelse:\n    print(\"‚úÖ Face detection model loaded successfully.\")\n\n# --- 2. Define Analysis Functions ---\n\ndef get_blur_score(frame):\n    \"\"\"\n    Calculates the blurriness of a single frame using Laplacian variance.\n    A higher score is SHARPER. A lower score is BLURRIER.\n    \"\"\"\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n    variance = cv2.Laplacian(gray, cv2.CV_64F).var()\n    return variance\n\ndef get_face_score(frame, gray_frame):\n    \"\"\"\n    Calculates a score based on the presence and size of faces.\n    Score is the total pixel area of all detected face bounding boxes.\n    \"\"\"\n    faces = face_cascade.detectMultiScale(\n        gray_frame, \n        scaleFactor=1.1, \n        minNeighbors=5, \n        minSize=(30, 30)\n    )\n    \n    total_face_area = 0\n    if len(faces) > 0:\n        for (x, y, w, h) in faces:\n            total_face_area += w * h\n    return total_face_area\n\ndef get_motion_score(prev_gray, next_gray):\n    \"\"\"\n    Calculates a motion score using optical flow.\n    A higher score means more movement/action (or shakiness).\n    \"\"\"\n    flow = cv2.calcOpticalFlowFarneback(\n        prev_gray, \n        next_gray, \n        None, \n        0.5, # pyr_scale\n        3,   # levels\n        15,  # winsize\n        3,   # iterations\n        5,   # poly_n\n        1.2, # poly_sigma\n        0    # flags\n    )\n    \n    # Calculate the magnitude (length) of the 2D flow vectors\n    magnitude, _ = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n    \n    # Return the average magnitude across the whole frame\n    return np.mean(magnitude)\n\n\n# --- 3. Main Processing Loop ---\n\nprint(f\"\\nStarting advanced analysis of {len(video_files)} files...\")\nall_shots_data = [] # Reset the list\n\nfor video_path in tqdm(video_files, desc=\"Processing Videos\"):\n    video_filename = os.path.basename(video_path)\n    \n    # This function is from your previous cell\n    shots = find_shots(video_path) \n    \n    if not shots:\n        continue\n        \n    cap = cv2.VideoCapture(video_path)\n    \n    for i, (start_time, end_time) in enumerate(shots):\n        start_frame = start_time.get_frames()\n        end_frame = end_time.get_frames()\n        \n        # We need two consecutive frames for motion analysis\n        middle_frame_num = int((start_frame + end_frame) / 2)\n        \n        # Read middle frame\n        cap.set(cv2.CAP_PROP_POS_FRAMES, middle_frame_num)\n        ret_mid, frame_mid = cap.read()\n        \n        # Read the frame right after it\n        cap.set(cv2.CAP_PROP_POS_FRAMES, middle_frame_num + 1)\n        ret_next, frame_next = cap.read()\n        \n        if ret_mid and ret_next and frame_mid is not None and frame_next is not None:\n            # Convert to grayscale\n            gray_mid = cv2.cvtColor(frame_mid, cv2.COLOR_BGR2GRAY)\n            gray_next = cv2.cvtColor(frame_next, cv2.COLOR_BGR2GRAY)\n            \n            # --- Analyze the frames ---\n            blur_score = get_blur_score(frame_mid)\n            face_score = get_face_score(frame_mid, gray_mid)\n            motion_score = get_motion_score(gray_mid, gray_next)\n            \n            # --- Store the results ---\n            shot_data = {\n                'video_file': video_filename,\n                'video_path': video_path,\n                'shot_num': i,\n                'start_timecode': start_time.get_timecode(),\n                'end_timecode': end_time.get_timecode(),\n                'duration_sec': (end_frame - start_frame) / start_time.framerate,\n                'blur_score': blur_score,\n                'face_score': face_score,\n                'motion_score': motion_score\n            }\n            all_shots_data.append(shot_data)\n        \n    cap.release()\n\nprint(\"\\n--- Advanced Analysis Complete ---\")\n\n# --- 4. Create the Final DataFrame ---\ndf_shots = pd.DataFrame(all_shots_data)\n\nif not df_shots.empty:\n    print(f\"‚úÖ Created DataFrame with {len(df_shots)} total shots.\")\n    \n    # Save the DataFrame to a CSV file for easy re-use\n    # This is a key part of our \"pipeline\"!\n    df_shots.to_csv(os.path.join(OUTPUT_DIR, \"shot_analysis_results.csv\"), index=False)\n    print(f\"‚úÖ Analysis data saved to {OUTPUT_DIR}shot_analysis_results.csv\")\n\n    # Let's look at the results\n    print(\"\\n--- Sample of Analysis Data ---\")\n    print(df_shots.head())\n    \n    print(\"\\n--- Top 5 'Best Face' Shots ---\")\n    print(df_shots.sort_values(by='face_score', ascending=False).head())\n    \n    print(\"\\n--- Top 5 'Highest Motion' Shots ---\")\n    print(df_shots.sort_values(by='motion_score', ascending=False).head())\nelse:\n    print(\"‚ö†Ô∏è Warning: The DataFrame is empty. No shots were analyzed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T13:26:03.104004Z","iopub.execute_input":"2025-11-01T13:26:03.104297Z","iopub.status.idle":"2025-11-01T13:28:19.761647Z","shell.execute_reply.started":"2025-11-01T13:26:03.104280Z","shell.execute_reply":"2025-11-01T13:28:19.760982Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Face detection model loaded successfully.\n\nStarting advanced analysis of 13 files...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing Videos:   0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1470861027c40218400765de6385533"}},"metadata":{}},{"name":"stdout","text":"Error processing /kaggle/input/short-clips/dataset1/frbnwkkq.mp4: 'tuple' object has no attribute 'get_frames'\n\n--- Advanced Analysis Complete ---\n‚úÖ Created DataFrame with 79 total shots.\n‚úÖ Analysis data saved to /kaggle/working/shot_analysis_results.csv\n\n--- Sample of Analysis Data ---\n     video_file                                       video_path  shot_num  \\\n0  asconvpt.mp4  /kaggle/input/short-clips/dataset1/asconvpt.mp4         0   \n1  asconvpt.mp4  /kaggle/input/short-clips/dataset1/asconvpt.mp4         1   \n2  asconvpt.mp4  /kaggle/input/short-clips/dataset1/asconvpt.mp4         2   \n3  asconvpt.mp4  /kaggle/input/short-clips/dataset1/asconvpt.mp4         3   \n4  blostjdq.mp4  /kaggle/input/short-clips/dataset1/blostjdq.mp4         0   \n\n  start_timecode  end_timecode  duration_sec  blur_score  face_score  \\\n0   00:00:00.000  00:00:20.760         20.76   64.161774        3364   \n1   00:00:20.760  00:00:59.300         38.54   89.137438           0   \n2   00:00:59.300  00:01:00.940          1.64   84.508437       54440   \n3   00:01:00.940  00:01:01.880          0.94  202.184176       56277   \n4   00:00:00.000  00:00:03.620          3.62  146.572058       46034   \n\n   motion_score  \n0      0.096856  \n1      0.422891  \n2      0.179704  \n3      0.089145  \n4      0.023640  \n\n--- Top 5 'Best Face' Shots ---\n      video_file                                       video_path  shot_num  \\\n45  unygqzdu.mp4  /kaggle/input/short-clips/dataset1/unygqzdu.mp4         3   \n48  wcsdambp.mp4  /kaggle/input/short-clips/dataset1/wcsdambp.mp4         1   \n47  wcsdambp.mp4  /kaggle/input/short-clips/dataset1/wcsdambp.mp4         0   \n7   blostjdq.mp4  /kaggle/input/short-clips/dataset1/blostjdq.mp4         3   \n16  chqblpze.mp4  /kaggle/input/short-clips/dataset1/chqblpze.mp4         6   \n\n   start_timecode  end_timecode  duration_sec  blur_score  face_score  \\\n45   00:00:10.960  00:00:13.100          2.14   65.858386      121481   \n48   00:00:03.780  00:00:07.380          3.60  663.589515      101964   \n47   00:00:00.000  00:00:03.780          3.78  711.802823       91792   \n7    00:00:21.080  00:00:29.920          8.84   27.893023       91205   \n16   00:01:28.300  00:01:59.640         31.34   83.400410       79979   \n\n    motion_score  \n45      0.053149  \n48      0.294788  \n47      0.425734  \n7       0.008349  \n16      1.324355  \n\n--- Top 5 'Highest Motion' Shots ---\n      video_file                                       video_path  shot_num  \\\n26  kjhjjxni.mp4  /kaggle/input/short-clips/dataset1/kjhjjxni.mp4         3   \n55  wcsdambp.mp4  /kaggle/input/short-clips/dataset1/wcsdambp.mp4         8   \n62  wuqoarpl.mp4  /kaggle/input/short-clips/dataset1/wuqoarpl.mp4         3   \n44  unygqzdu.mp4  /kaggle/input/short-clips/dataset1/unygqzdu.mp4         2   \n53  wcsdambp.mp4  /kaggle/input/short-clips/dataset1/wcsdambp.mp4         6   \n\n   start_timecode  end_timecode  duration_sec  blur_score  face_score  \\\n26   00:00:07.620  00:00:23.760         16.14  528.953038       14815   \n55   00:00:14.380  00:00:18.180          3.80  326.462902       27222   \n62   00:00:20.480  00:00:22.520          2.04  342.674898           0   \n44   00:00:05.140  00:00:10.960          5.82  544.630886       23419   \n53   00:00:11.960  00:00:13.560          1.60  321.430785           0   \n\n    motion_score  \n26      3.400924  \n55      3.220973  \n62      3.081489  \n44      3.017061  \n53      2.313847  \n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# --- Stage 2: Scoring, Filtering, and Selection ---\nimport pandas as pd\nimport numpy as np\nimport os\n\n# --- 1. Load our Analysis Data ---\n# This proves our pipeline is modular. We don't have to re-run Stage 1.\nDATA_FILE = os.path.join(OUTPUT_DIR, \"shot_analysis_results.csv\")\n\nif not os.path.exists(DATA_FILE):\n    print(\"‚ö†Ô∏è Error: shot_analysis_results.csv not found!\")\n    print(\"Please re-run the previous analysis cell (Step 1.3).\")\nelse:\n    print(f\"‚úÖ Successfully loaded {DATA_FILE}\")\n    df_shots = pd.read_csv(DATA_FILE)\n\n# --- 2. Normalize the Data (Feature Scaling) ---\n# We scale all scores from 0 to 1 so we can combine them fairly.\n# (e.g., a face_score of 90k isn't 90,000x more important than a blur_score of 100)\ndf_shots['blur_norm'] = (df_shots['blur_score'] - df_shots['blur_score'].min()) / (df_shots['blur_score'].max() - df_shots['blur_score'].min())\ndf_shots['face_norm'] = (df_shots['face_score'] - df_shots['face_score'].min()) / (df_shots['face_score'].max() - df_shots['face_score'].min())\ndf_shots['motion_norm'] = (df_shots['motion_score'] - df_shots['motion_score'].min()) / (df_shots['motion_score'].max() - df_shots['motion_score'].min())\n\nprint(\"‚úÖ Data normalized (scaled from 0 to 1).\")\n\n\n# --- 3. Define the \"Interest Score\" Algorithm ---\n# This is the \"secret sauce\" for the \"technical depth\" criterion.\n# We can tune these weights to change the style of the final video.\nW_FACE = 1.5   # We really care about faces\nW_SHARPNESS = 1.0   # We like sharp shots\nW_MOTION = 0.5   # We like *some* motion, but it's less important\nW_DURATION = 0.1 # We slightly prefer longer shots over 1-second cuts\n\ndef calculate_interest_score(row):\n    score = (row['face_norm'] * W_FACE) \\\n          + (row['blur_norm'] * W_SHARPNESS) \\\n          + (row['motion_norm'] * W_MOTION) \\\n          + (np.log1p(row['duration_sec']) * W_DURATION) # Use log for duration\n    return score\n\ndf_shots['interest_score'] = df_shots.apply(calculate_interest_score, axis=1)\nprint(\"‚úÖ 'interest_score' calculated.\")\n\n\n# --- 4. Filter Out \"Bad\" Shots ---\n# These are hard-coded rules to ensure \"Quality\" (a judging criterion).\n# We will REMOVE any shot that...\nMIN_BLUR_THRESHOLD = 50.0  # Is too blurry (based on our 1.2 output)\nMIN_DURATION_SEC = 1.5     # Is too short to be cinematic\nMAX_MOTION_SCORE = 3.0     # Is *too* shaky (based on our 1.3 output)\n\n# Create a 'rejection_reason' column to explain our logic\ndef get_rejection_reason(row):\n    if row['blur_score'] < MIN_BLUR_THRESHOLD:\n        return \"Too Blurry\"\n    if row['duration_sec'] < MIN_DURATION_SEC:\n        return \"Too Short\"\n    if row['motion_score'] > MAX_MOTION_SCORE:\n        return \"Too Shaky\"\n    return None # Keep this shot\n\ndf_shots['rejection_reason'] = df_shots.apply(get_rejection_reason, axis=1)\n\n# Create our final list of \"good\" shots to choose from\ndf_finalist_shots = df_shots[df_shots['rejection_reason'].isnull()].copy()\ndf_rejected_shots = df_shots[df_shots['rejection_reason'].notnull()].copy()\n\nprint(f\"‚úÖ Filtering complete.\")\nprint(f\"  -> {len(df_shots)} total shots analyzed.\")\nprint(f\"  -> {len(df_rejected_shots)} shots rejected.\")\nprint(f\"  -> {len(df_finalist_shots)} 'good' shots remaining.\")\n\n\n# --- 5. Select the \"Best\" Shots ---\n# We sort by our score and pick the top ones until we hit our time limit.\n# (Hackathon requirement: 5-10 minutes)\nTARGET_DURATION_SEC = 8 * 60  # Let's aim for 8 minutes\n\n# Sort by best score\ndf_finalist_shots = df_finalist_shots.sort_values(by='interest_score', ascending=False)\n\n# Add shots to our \"final edit list\" one by one\nselected_shots = []\ntotal_duration = 0\nfor index, row in df_finalist_shots.iterrows():\n    if total_duration + row['duration_sec'] <= TARGET_DURATION_SEC:\n        selected_shots.append(row)\n        total_duration += row['duration_sec']\n\ndf_edit_list = pd.DataFrame(selected_shots)\n\nprint(f\"\\n--- Final Selection Complete ---\")\nprint(f\"‚úÖ Selected {len(df_edit_list)} shots.\")\nprint(f\"‚úÖ Final video duration: {total_duration / 60:.2f} minutes.\")\n\nprint(\"\\n--- Top 10 Shots Selected for Final Video ---\")\nprint(df_edit_list[['video_file', 'duration_sec', 'interest_score', 'face_score', 'blur_score', 'motion_score']].head(10))\n\nprint(\"\\n--- Top 5 Shots that were REJECTED ---\")\nprint(df_rejected_shots[['video_file', 'duration_sec', 'rejection_reason', 'blur_score', 'motion_score']].head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T13:30:00.178118Z","iopub.execute_input":"2025-11-01T13:30:00.178459Z","iopub.status.idle":"2025-11-01T13:30:00.217081Z","shell.execute_reply.started":"2025-11-01T13:30:00.178435Z","shell.execute_reply":"2025-11-01T13:30:00.216434Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Successfully loaded /kaggle/working/shot_analysis_results.csv\n‚úÖ Data normalized (scaled from 0 to 1).\n‚úÖ 'interest_score' calculated.\n‚úÖ Filtering complete.\n  -> 79 total shots analyzed.\n  -> 28 shots rejected.\n  -> 51 'good' shots remaining.\n\n--- Final Selection Complete ---\n‚úÖ Selected 50 shots.\n‚úÖ Final video duration: 7.74 minutes.\n\n--- Top 10 Shots Selected for Final Video ---\n      video_file  duration_sec  interest_score  face_score  blur_score  \\\n48  wcsdambp.mp4          3.60        2.145333      101964  663.589515   \n47  wcsdambp.mp4          3.78        2.093474       91792  711.802823   \n45  unygqzdu.mp4          2.14        1.685162      121481   65.858386   \n71  wuqoarpl.mp4         16.28        1.671743       59693  558.334193   \n41  ukonzgxq.mp4         32.42        1.641155       68046  381.143520   \n73  wuqoarpl.mp4          3.34        1.631759       61876  556.101346   \n16  chqblpze.mp4         31.34        1.611682       79979   83.400410   \n38  ohwchuju.mp4          6.24        1.591734       62200  524.052653   \n29  myqiuzzc.mp4          9.98        1.511220       50919  582.137931   \n74  wuqoarpl.mp4          2.94        1.475753       59229  543.352259   \n\n    motion_score  \n48      0.294788  \n47      0.425734  \n45      0.053149  \n71      0.474458  \n41      0.381411  \n73      0.973660  \n16      1.324355  \n38      0.555945  \n29      0.258544  \n74      0.293300  \n\n--- Top 5 Shots that were REJECTED ---\n      video_file  duration_sec rejection_reason  blur_score  motion_score\n3   asconvpt.mp4          0.94        Too Short  202.184176      0.089145\n5   blostjdq.mp4         11.54       Too Blurry   14.324140      1.416547\n7   blostjdq.mp4          8.84       Too Blurry   27.893023      0.008349\n9   blostjdq.mp4          0.80        Too Short  473.645380      0.076251\n18  chqblpze.mp4          1.26        Too Short  292.294421      0.155175\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# --- Stage 3 & 4: Audio Upload and Final Video Assembly (CORRECTED) ---\n\n# --- 1. Import all required libraries ---\nfrom moviepy.editor import (VideoFileClip, concatenate_videoclips, AudioFileClip, \n                            CompositeVideoClip)\nimport moviepy.video.fx.all as vfx\nfrom moviepy.audio.fx.all import audio_loop\nimport os\nfrom tqdm.notebook import tqdm\n\n# --- 3.1: Load Your Uploaded Music ---\n\n# --- FIX 1: Using the correct path from your Dataset ---\nMUSIC_FILE = \"/kaggle/input/musicc/my-cool-music.mp3\"\n# --- END FIX ---\n\nmusic = None\nif not os.path.exists(MUSIC_FILE):\n    print(f\"‚ö†Ô∏è Music file not found at {MUSIC_FILE}\")\n    print(\"Please double-check the 'musicc' dataset name.\")\nelse:\n    try:\n        music = AudioFileClip(MUSIC_FILE)\n        print(f\"‚úÖ Successfully loaded music: {MUSIC_FILE}\")\n    except Exception as e:\n        print(f\"‚ö†Ô∏è Could not load audio file: {e}. The final video will be silent.\")\n\n\n# --- 4.1: Assemble Final Video Clips ---\nprint(\"\\n--- Starting Final Video Assembly ---\")\nprint(f\"Loading and trimming {len(df_edit_list)} selected shots...\")\n\nTRANSITION_SEC = 0.5 \nfinal_clips_list = []\n\n# --- FIX 2: Fixed the 'NoneType' bug ---\n# We will keep parent clips in this list to close them LATER\nparent_clips_to_close = []\n# --- END FIX ---\n\nfor index, row in tqdm(df_edit_list.iterrows(), total=len(df_edit_list)):\n    try:\n        clip = VideoFileClip(row['video_path'])\n        parent_clips_to_close.append(clip) # Add parent clip to our new list\n        \n        clip_trimmed = clip.subclip(row['start_timecode'], row['end_timecode'])\n        clip_trimmed = clip_trimmed.set_audio(None)\n        clip_with_fade = clip_trimmed.fx(vfx.fadein, TRANSITION_SEC)\n        \n        final_clips_list.append(clip_with_fade)\n        \n        # --- REMOVED `clip.close()` FROM HERE ---\n        \n    except Exception as e:\n        print(f\"‚ö†Ô∏è Warning: Failed to process shot {row['video_file']} at {row['start_timecode']}. Error: {e}. Skipping.\")\n        \nprint(f\"‚úÖ {len(final_clips_list)} clips prepared.\")\n\n\n# --- 4.2: Concatenate with Transitions ---\nif final_clips_list:\n    final_video = concatenate_videoclips(final_clips_list, \n                                         padding = -TRANSITION_SEC, \n                                         method=\"compose\")\n\n    # --- 4.3: Add Background Music ---\n    if music:\n        print(\"Adding background music...\")\n        # Your video is 7.74 min and music is 5.5 min, so we'll loop it.\n        if music.duration < final_video.duration:\n            print(\"Music is shorter than video, looping audio...\")\n            music = audio_loop(music, duration=final_video.duration)\n        \n        music = music.subclip(0, final_video.duration)\n        final_video = final_video.set_audio(music)\n\n    # --- 4.4: Render Final Video ---\n    FINAL_VIDEO_PATH = os.path.join(OUTPUT_DIR, \"hackathon_submission.mp4\")\n    print(f\"\\n--- üöÄ Rendering Final Video ---\")\n    print(f\"This will take several minutes...\")\n    \n    try:\n        final_video.write_videofile(FINAL_VIDEO_PATH, \n                                    codec='libx264', \n                                    audio_codec='aac', \n                                    threads=4, \n                                    preset='medium')\n        \n        print(f\"\\nüéâüéâüéâ --- SUCCESS! --- üéâüéâüéâ\")\n        print(f\"Your final cinematic video is saved to:\")\n        print(FINAL_VIDEO_PATH)\n        print(\"You can download it from the 'Output' section on the right-hand panel.\")\n        \n    except Exception as e:\n        print(f\"üî•üî•üî• Rendering Failed: {e}\")\n\n    # --- 4.5: Final Cleanup ---\n    print(\"Cleaning up file handles...\")\n    if music:\n        music.close()\n    for clip in final_clips_list:\n        clip.close()\n    final_video.close()\n    \n    # --- FIX 2 (Continued): Now we close the parent clips ---\n    for clip in parent_clips_to_close:\n        clip.close()\n    print(\"‚úÖ Cleanup complete.\")\n    \nelse:\n    print(\"‚ö†Ô∏è No clips were prepared. Final video not created.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T14:03:27.968087Z","iopub.execute_input":"2025-11-01T14:03:27.968667Z","iopub.status.idle":"2025-11-01T14:15:16.054925Z","shell.execute_reply.started":"2025-11-01T14:03:27.968638Z","shell.execute_reply":"2025-11-01T14:15:16.054164Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Successfully loaded music: /kaggle/input/musicc/my-cool-music.mp3\n\n--- Starting Final Video Assembly ---\nLoading and trimming 50 selected shots...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/50 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdf5f94fcbcf4c77b15b819ab5aca85d"}},"metadata":{}},{"name":"stdout","text":"‚úÖ 50 clips prepared.\nAdding background music...\nMusic is shorter than video, looping audio...\n\n--- üöÄ Rendering Final Video ---\nThis will take several minutes...\nMoviepy - Building video /kaggle/working/hackathon_submission.mp4.\nMoviePy - Writing audio in hackathon_submissionTEMP_MPY_wvf_snd.mp4\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"name":"stdout","text":"MoviePy - Done.\nMoviepy - Writing video /kaggle/working/hackathon_submission.mp4\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"Moviepy - Done !\nMoviepy - video ready /kaggle/working/hackathon_submission.mp4\n\nüéâüéâüéâ --- SUCCESS! --- üéâüéâüéâ\nYour final cinematic video is saved to:\n/kaggle/working/hackathon_submission.mp4\nYou can download it from the 'Output' section on the right-hand panel.\nCleaning up file handles...\n‚úÖ Cleanup complete.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}